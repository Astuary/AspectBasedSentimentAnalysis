{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_DK_IMDB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1fdawjr4FLCySyj210tDbf8DBd-1jlb0z",
      "authorship_tag": "ABX9TyN4/hnyU61YlR9bBEnTUbeJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Astuary/AspectBasedSentimentAnalysis/blob/main/BERT_DK_IMDB_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FWZQZdzkUKE",
        "outputId": "f16d164a-5c3d-43b1-af48-a49cb1f2d03f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os\n",
        "for dirname, _, filenames in os.walk('drive/My Drive/Colab Notebooks'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive/My Drive/Colab Notebooks/train.csv\n",
            "drive/My Drive/Colab Notebooks/test.csv\n",
            "drive/My Drive/Colab Notebooks/Copy of CS685_HW0.ipynb\n",
            "drive/My Drive/Colab Notebooks/CS685_HW0_Kunjal_Panchal.ipynb\n",
            "drive/My Drive/Colab Notebooks/CS685 Quiz 1: Neural LMs.gdoc\n",
            "drive/My Drive/Colab Notebooks/CS685 Quiz 2: Attention.gdoc\n",
            "drive/My Drive/Colab Notebooks/CS685 Quiz 3: Transformers   transfer.gdoc\n",
            "drive/My Drive/Colab Notebooks/MLFL (24th September) - John Wieting (CMU   Google Research).gdoc\n",
            "drive/My Drive/Colab Notebooks/CS685 Quiz 4: BERT MLM.gdoc\n",
            "drive/My Drive/Colab Notebooks/CS685 Quiz 5: text generation.gdoc\n",
            "drive/My Drive/Colab Notebooks/CS685_HW1.ipynb\n",
            "drive/My Drive/Colab Notebooks/data_distribute.npz\n",
            "drive/My Drive/Colab Notebooks/BERT-PT-REST-TRIAL.ipynb\n",
            "drive/My Drive/Colab Notebooks/NN.ipynb\n",
            "drive/My Drive/Colab Notebooks/IMDB Dataset.csv.zip\n",
            "drive/My Drive/Colab Notebooks/IMDB Dataset.csv\n",
            "drive/My Drive/Colab Notebooks/BERT_XD_REVIEW_IMDB.ipynb\n",
            "drive/My Drive/Colab Notebooks/BERT_DK_IMDB.ipynb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4Qm3nYQpXrL",
        "outputId": "21310f60-bcb9-4c14-e3fa-750070fa5da7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import sys\n",
        "import numpy as np\n",
        "import random as rn\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.optim import Adam\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "! pip install transformers\n",
        "from transformers import AutoModel, AutoTokenizer, AutoModelForSequenceClassification, BertTokenizer, AdamW"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (3.4.0)\n",
            "Requirement already satisfied: tokenizers==0.9.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.94)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.6/dist-packages (from transformers) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf->transformers) (50.3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIhUpK34qBKG"
      },
      "source": [
        "rn.seed(1)\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "torch.cuda.manual_seed(1)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGlJp_R8qNT5"
      },
      "source": [
        "path = 'drive/My Drive/Colab Notebooks/'\n",
        "train_data = pd.read_csv(path + 'train.csv')\n",
        "test_data = pd.read_csv(path + 'test.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXrLd5meqiN7",
        "outputId": "1ee2a3d5-acb4-4a87-e1ce-c8dec0a60b30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_data = train_data#[:2000]\n",
        "test_data = test_data#[:500]\n",
        "\n",
        "train_data = train_data.to_dict(orient='records')\n",
        "test_data = test_data.to_dict(orient='records')\n",
        "type(train_data)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBSSbvxZqm6J",
        "outputId": "164cb99e-d126-46bf-f62f-00ae6c0e8e61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "train_texts, train_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), train_data)))\n",
        "test_texts, test_labels = list(zip(*map(lambda d: (d['text'], d['sentiment']), test_data)))\n",
        "\n",
        "len(train_texts), len(train_labels), len(test_texts), len(test_labels)\n",
        "train_texts[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"Now, I won't deny that when I purchased this off eBay, I had high expectations. This was an incredible out-of-print work from the master of comedy that I so enjoy. However, I was soon to be disappointed. Apologies to those who enjoyed it, but I just found the Compleat Al to be very difficult to watch. I got a few smiles, sure, but the majority of the funny came from the music videos (which I've got on DVD) and the rest was basically filler. You could tell that this was not Al's greatest video achievement (that honor goes to UHF). Honestly, I doubt if this will ever make the jump to DVD, so if you're an ultra-hardcore Al fan and just HAVE to own everything, buy the tape off eBay. Just don't pay too much for it.\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYw4jHMEqoRN"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('activebus/BERT-DK_rest')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "12KDNt7irAAP",
        "outputId": "7bdf6fb5-0216-47b5-f7ac-c0ce49ded082",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], train_texts))\n",
        "test_tokens = list(map(lambda t: ['[CLS]'] + tokenizer.tokenize(t)[:510] + ['[SEP]'], test_texts))\n",
        "\n",
        "len(train_tokens), len(test_tokens)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 25000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW0UQrrarRDY",
        "outputId": "1fe5dda9-bba2-443d-ef25-60575e4b200e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, train_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "test_tokens_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, test_tokens)), maxlen=512, truncating=\"post\", padding=\"post\", dtype=\"int\")\n",
        "\n",
        "train_tokens_ids.shape, test_tokens_ids.shape"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((25000, 512), (25000, 512))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjsdLVNrrUUg",
        "outputId": "ed213e20-81ec-42c1-8aa0-5b9b195f622d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_y = np.array(train_labels) == 'pos'\n",
        "test_y = np.array(test_labels) == 'pos'\n",
        "train_y.shape, test_y.shape, np.mean(train_y), np.mean(test_y)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((25000,), (25000,), 0.5, 0.5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1v6KxwssM0X"
      },
      "source": [
        "train_masks = [[float(i > 0) for i in ii] for ii in train_tokens_ids]\n",
        "test_masks = [[float(i > 0) for i in ii] for ii in test_tokens_ids]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9X3MsgbrriRx"
      },
      "source": [
        "class BertBinaryClassifier(nn.Module):\n",
        "    def __init__(self, dropout=0.1):\n",
        "        super(BertBinaryClassifier, self).__init__()\n",
        "\n",
        "        self.bert = AutoModel.from_pretrained('activebus/BERT-DK_rest')\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.linear = nn.Linear(768, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def forward(self, tokens, masks=None):\n",
        "        _, pooled_output = self.bert(tokens, attention_mask=masks)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output = self.linear(dropout_output)\n",
        "        proba = self.sigmoid(linear_output)\n",
        "        return proba"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjiVjblwrpEu",
        "outputId": "77b4d4df-bf97-40bb-8241-acd1d10470a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "bert_clf = BertBinaryClassifier()\n",
        "bert_clf = bert_clf.cuda() \n",
        "\n",
        "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'439.069184M'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_kQyUevr9dn"
      },
      "source": [
        "BATCH_SIZE = 4\n",
        "EPOCHS = 10"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xX-35vWhsAh6",
        "outputId": "a381f6d5-6a63-4e99-e702-fc3e32f9386c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_tokens_tensor = torch.tensor(train_tokens_ids)\n",
        "train_y_tensor = torch.tensor(train_y.reshape(-1, 1)).float()\n",
        "\n",
        "test_tokens_tensor = torch.tensor(test_tokens_ids)\n",
        "test_y_tensor = torch.tensor(test_y.reshape(-1, 1)).float()\n",
        "\n",
        "train_masks_tensor = torch.tensor(train_masks)\n",
        "test_masks_tensor = torch.tensor(test_masks)\n",
        "\n",
        "str(torch.cuda.memory_allocated(device)/1000000 ) + 'M'"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1765.728256M'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0u19go1sU3t"
      },
      "source": [
        "train_dataset = TensorDataset(train_tokens_tensor, train_masks_tensor, train_y_tensor)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=BATCH_SIZE)\n",
        "\n",
        "test_dataset = TensorDataset(test_tokens_tensor, test_masks_tensor, test_y_tensor)\n",
        "test_sampler = SequentialSampler(test_dataset)\n",
        "test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxGVzz1LsYDA"
      },
      "source": [
        "param_optimizer = list(bert_clf.sigmoid.named_parameters()) \n",
        "optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KSGXZ13sa5f"
      },
      "source": [
        "optimizer = Adam(bert_clf.parameters(), lr=3e-6)\n",
        "\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayNIJZHHsjpc",
        "outputId": "406c1012-7b9b-40e1-cade-104a5e27ad11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for epoch_num in range(EPOCHS):\n",
        "    bert_clf.train()\n",
        "    train_loss = 0\n",
        "    for step_num, batch_data in enumerate(train_dataloader):\n",
        "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "        print(str(torch.cuda.memory_allocated(device)/1000000 ) + 'M')\n",
        "        logits = bert_clf(token_ids, masks=masks)\n",
        "        \n",
        "        loss_func = nn.BCELoss()\n",
        "\n",
        "        batch_loss = loss_func(logits, labels)\n",
        "        train_loss += batch_loss.item()\n",
        "        \n",
        "        \n",
        "        bert_clf.zero_grad()\n",
        "        batch_loss.backward()\n",
        "        \n",
        "\n",
        "        clip_grad_norm_(parameters=bert_clf.parameters(), max_norm=1.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        clear_output(wait=True)\n",
        "        print('Epoch: ', epoch_num + 1)\n",
        "        print(\"\\r\" + \"{0}/{1} loss: {2} \".format(step_num, len(train_data) / BATCH_SIZE, train_loss / (step_num + 1)))\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:  10\n",
            "\r6249/6250.0 loss: 0.022619232236823373 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1Gfgtgvt1oV"
      },
      "source": [
        "bert_clf.eval()\n",
        "bert_predicted = []\n",
        "all_logits = []\n",
        "with torch.no_grad():\n",
        "    for step_num, batch_data in enumerate(test_dataloader):\n",
        "\n",
        "        token_ids, masks, labels = tuple(t.to(device) for t in batch_data)\n",
        "\n",
        "        logits = bert_clf(token_ids, masks)\n",
        "        loss_func = nn.BCELoss()\n",
        "        loss = loss_func(logits, labels)\n",
        "        numpy_logits = logits.cpu().detach().numpy()\n",
        "        \n",
        "        bert_predicted += list(numpy_logits[:, 0] > 0.5)\n",
        "        all_logits += list(numpy_logits[:, 0])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_affD95t7zq",
        "outputId": "3f54b147-67d4-4d57-d54c-0314a4e814b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(test_y, bert_predicted))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.95      0.93      0.94     12500\n",
            "        True       0.94      0.95      0.94     12500\n",
            "\n",
            "    accuracy                           0.94     25000\n",
            "   macro avg       0.94      0.94      0.94     25000\n",
            "weighted avg       0.94      0.94      0.94     25000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv1TntFbWJeJ",
        "outputId": "8ba396a8-1bf6-436e-b81e-00d9066ae379",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "torch.save(bert_clf.state_dict(), \"drive/My Drive/Colab Notebooks/BERT_DK_REVIEW.pth\")\n",
        "tokenizer.save_pretrained(\"drive/My Drive/Colab Notebooks/BERT_DK_REVIEW_TKN\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('drive/My Drive/Colab Notebooks/BERT_DK_REVIEW_TKN/tokenizer_config.json',\n",
              " 'drive/My Drive/Colab Notebooks/BERT_DK_REVIEW_TKN/special_tokens_map.json',\n",
              " 'drive/My Drive/Colab Notebooks/BERT_DK_REVIEW_TKN/vocab.txt',\n",
              " 'drive/My Drive/Colab Notebooks/BERT_DK_REVIEW_TKN/added_tokens.json')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    }
  ]
}